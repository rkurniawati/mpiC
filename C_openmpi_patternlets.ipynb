{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXMYxZtKyP0I"
      },
      "source": [
        "# Distributed Parallel Programming Patterns using Open MPI\n",
        "C adaptation done by Ruth Kurniawati (Westfield State University) using source code from [CSInParallel](https://github.com/csinparallel/CSinParallel.git)\n",
        "\n",
        "Modified from mpi4py notebook originally written by Libby Shoop, Macalester College\n",
        "\n",
        "Welcome!\n",
        "\n",
        "This book contains some examples illustrating the basic fundamental concepts of distributed computing using C code. The type of computing these examples illustrate is called *message passing*. Message passing is a form of programming that is based on processes that communicate with each other to coordinate their work. Message passing can be used on a single multicore computer or with a cluster of computers.\n",
        "\n",
        "### Software Patterns\n",
        "\n",
        "Patterns in software are common implementations that have been used over and over by practitioners to accomplish tasks. As practitioners use them repeatedly, the community begins to give them names and catalog them, often turning them into reusable library functions. The examples you will see in this book are based on documented patterns that have been used to solve different problems using message passing between processes. Message passing is one form of distributed computing using processes, which can be used on clusters of computers or multicore machines.\n",
        "\n",
        "In many of these examples, the pattern's name is part of the C code file's name. You will also see that often the MPI library functions also take on the name of the pattern, and the implementation of those functions themselves contains the pattern that practitioners found themselves using often. These pattern code examples we show you here, dubbed patternlets, are based on original work by Joel Adams:\n",
        "\n",
        "Adams, Joel C. \"Patternlets: A Teaching Tool for Introducing Students to Parallel Design Patterns.\" 2015 IEEE International Parallel and Distributed Processing Symposium Workshop. IEEE, 2015.\n",
        "\n",
        "To run these examples, first you may need to install openmpi by running this code. You may need to re-run this if the notebook has been disconnected from the host runtime and has to be restarted. You don't need to run this code if you open this notebook in Google Colab or Binder using one of the links from the README file. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVgDVtdkxrgc",
        "outputId": "47a671a6-cff0-4b48-935a-e4b2793f5468"
      },
      "outputs": [],
      "source": [
        "!apt install -y openmpi-bin libopenmpi-dev"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ4ul5SKsfmM"
      },
      "source": [
        "### New to colab and jupyter notebook?\n",
        "\n",
        "If you have not used this type of notebook before, these are split into *cells*. The cell you are reading is a text cell, and the cell just above it is also. the cell with [ ] to the left of it is a code cell, which contains C code or code that can be run as if you are in a linux shell. The latter linux shell commands always begin with an exclamation point, !, as the cell above that contains a wget command, used to download the mpi.jar file.\n",
        "\n",
        "You should execute code cells as you follow along in this notebook. Some are designed for you to re-run after changing them. You can run a cell by hovering over the [ ] and clicking on the arrow symbol.\n",
        "\n",
        "If you open this notebook in Google Colab, the hamburger icon in the upper left (the one that looks like three __ symbols), toggles the table of contents. Revealing this enables you to navigate to different pattern examples.\n",
        "\n",
        "The triangle next to some text cells below enables collapsing of sections for faster scrolling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uryzb1Wy-hlp"
      },
      "source": [
        "# Program structure patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdWhWJMLzUIm"
      },
      "source": [
        "## Single Program, Multiple Data\n",
        "\n",
        "This code forms the basis of all of the other examples that follow. It is the fundamental way we structure parallel programs today.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgRHUlUsSN5u",
        "outputId": "a43e537e-25a1-4d00-ff54-67ab03777d0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing spmd.c\n"
          ]
        }
      ],
      "source": [
        "%%writefile spmd.c\n",
        "/* spmd.c\n",
        " * ... illustrates the single program multiple data\n",
        " *      (SPMD) pattern using basic MPI commands.\n",
        " *\n",
        " * Joel Adams, Calvin College, November 2009.\n",
        " *\n",
        " * Usage: mpirun -np 4 ./spmd\n",
        " *\n",
        " * Exercise:\n",
        " * - Compile and run.\n",
        " * - Compare source code to output.\n",
        " * - Rerun, using varying numbers of processes\n",
        " *    (i.e., vary the argument to 'mpirun -np').\n",
        " * - Explain what \"multiple data\" values this\n",
        " *    \"single program\" is generating.\n",
        " */\n",
        "\n",
        "#include <stdio.h>   // printf()\n",
        "#include <mpi.h>     // MPI functions\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    int id = -1, numProcesses = -1, length = -1;\n",
        "    char myHostName[MPI_MAX_PROCESSOR_NAME];\n",
        "\n",
        "    MPI_Init(&argc, &argv);\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &id);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n",
        "    MPI_Get_processor_name (myHostName, &length);\n",
        "\n",
        "    printf(\"Greetings from process #%d of %d on %s\\n\",\n",
        "             id, numProcesses, myHostName);\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRn94fx0WB3h"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLFiYTIJ6ALs"
      },
      "source": [
        "Let's examine the variables created in lines 18-29 carefully.\n",
        "\n",
        "1. *comm* The fundamental notion with this type of computing is a *process* running independently on the computer. With one single program like this, we can specify that we want to start several processes, each of which can **communicate**. The mechanism for communication is initialized when the program starts up, and the object that represents the means of using communication between processes is called MPI.COMM_WORLD.\n",
        "\n",
        "2. *id* Every process can identify itself with a number. We get that number by asking *comm* for it using Get_rank().\n",
        "\n",
        "3. *numProcesses* It is helpful to know haw many processes have started up, because this can be specified differently every time you run this type of program. Asking *comm* for it is done with Get_size().\n",
        "\n",
        "4. *myHostName* When you run this code on a cluster of computers, it is sometimes useful to know which computer is running a certain piece of code. A particular computer is often called a 'host', which is why we call this variable myHostName, and get it by asking *comm* to provide it with Get_processor_name().\n",
        "\n",
        "These four variables are often used in every MPI program. The first three are often needed for writing correct programs, and the fourth one is often used for debugging and analysis of where certain computations are running.\n",
        "\n",
        "Next we see how we can compile and use the mpirun program to execute the above C code using 4 processes. The C code will be saved when you execute the previous cell. The next cell will compile and run the saved C source code. We then run the program using mpirun -- the value after -np is the number of processes to use when running the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqOAtb4G4-e2",
        "outputId": "fb0e7030-b83d-4ce6-da04-786c4e34fc6b"
      },
      "outputs": [],
      "source": [
        "!mpicc -Wall -o spmd spmd.c\n",
        "!mpirun --allow-run-as-root -np 4 ./spmd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR2tfQ8v8RVa"
      },
      "source": [
        "The fundamental idea of message passing programs can be illustrated like this:\n",
        "\n",
        "![picture](images/comm_world.png)\n",
        "\n",
        "Each process is set up within a communication network to be able to communicate with every other process via communication links. Each process is set up to have its own number, or id, which starts at 0.\n",
        "\n",
        "**Note:** Each process holds its own copies of the above 4 data variables. **So even though there is one single program, it is running multiple times in separate processes, each holding its own data values.** This is the reason for the name of the pattern this code represents: single program, multiple data. The print line at the end of main() represents the multiple different data output produced by each process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSEhP3vv-_yX"
      },
      "source": [
        "## Master-Worker\n",
        "This is also a very common pattern used in parallel and distributed programming. Here's the sample small illustrative code. Review it and answer this: What is different between this example and the previous one?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlF-_TYc_uKu",
        "outputId": "a00e390d-f76d-41e1-eb7b-d9388abeb592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing masterWorker.c\n"
          ]
        }
      ],
      "source": [
        "%%writefile masterWorker.c\n",
        "/* masterWorker.c\n",
        " * ... illustrates the basic master-worker pattern in MPI ...\n",
        " * Joel Adams, Calvin College, November 2009.\n",
        " *\n",
        " * Usage: mpirun -np N ./masterWorker\n",
        " *\n",
        " * Exercise:\n",
        " * - Compile and run the program, varying N from 1 through 8.\n",
        " * - Explain what stays the same and what changes as the\n",
        " *    number of processes changes.\n",
        " */\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <mpi.h>\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  int id = -1, numWorkers = -1, length = -1;\n",
        "  char hostName[MPI_MAX_PROCESSOR_NAME];\n",
        "\n",
        "  MPI_Init(&argc, &argv);\n",
        "  MPI_Comm_rank(MPI_COMM_WORLD, &id);\n",
        "  MPI_Comm_size(MPI_COMM_WORLD, &numWorkers);\n",
        "  MPI_Get_processor_name (hostName, &length);\n",
        "\n",
        "  if ( id == 0 ) {  // process 0 is the master \n",
        "    printf(\"Greetings from the master, #%d (%s) of %d processes\\n\",\n",
        "             id, hostName, numWorkers);\n",
        "  } else {          // processes with ids > 0 are workers \n",
        "    printf(\"Greetings from a worker, #%d (%s) of %d processes\\n\",\n",
        "             id, hostName, numWorkers);\n",
        "  }\n",
        "\n",
        "  MPI_Finalize();\n",
        "  return 0;\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ0rjAxS_9xK"
      },
      "source": [
        "The answer to the above question illustrates what we can do with this pattern: based on the process id, we can have one process carry out something different than the others. This concept is used a lot as a means to coordinate activities, where one process, often called the master, has the responsibility of handing out work and keeping track of results. We will see this in later examples.\n",
        "\n",
        "**Note:** By convention, the master coordinating process is usually the process number 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8HeRMx-APS2",
        "outputId": "2078fe6f-e866-4b84-91ac-506b2a3c0af9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greetings from a worker, #1 (ip-172-31-58-111) of 2 processes\n",
            "Greetings from the master, #0 (ip-172-31-58-111) of 2 processes\n"
          ]
        }
      ],
      "source": [
        "!mpicc -Wall -ansi -pedantic -std=c99 -o masterWorker masterWorker.c\n",
        "!mpirun --allow-run-as-root -np 4 ./masterWorker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Eyj6sa7GoXu"
      },
      "source": [
        "### Exercises:\n",
        "\n",
        "- Rerun, using varying numbers of processes from 1 through 8 (i.e., vary the argument after -np).\n",
        "- Explain what stays the same and what changes as the number of processes changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1uIEnUS-JGG"
      },
      "source": [
        "# Decomposition using parallel for loop patterns\n",
        "\n",
        "The most common way to complete a repeated task in any program language is a loop. We use loops because we want to do a certain number of tasks, very often because we want to work on a set of data elements found in a list or an array, or some other data structure. If the work to be done in each loop is independent of previous iterations, we can use separate processes to do parts of the loop independently. This program structure pattern is called the parallel for loop pattern, which is an implementation strategy for decomposition of the work to be done into smaller parts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVWqN3EhJFbJ"
      },
      "source": [
        "## Parallel Loop Split into Equal Sized Chunks\n",
        "\n",
        "In the code below, notice the use of the variable called `REPS`. This is designed to be the total amount or work, or repetitions, that the for loop is accomplishing. This particular code is designed so that if those repetitions do not divide equally by the number of processes, then the program will stop with a warning message printed by the master process.\n",
        "\n",
        "Remember that because this is still also a SPMD program, all processes execute the code in the part of the if statement that evaluates to True. Each process has its own id, and we can determine how many processes there are, so we can choose where in the overall number of REPs of the loop each process will execute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OpxV7pnJ_u2",
        "outputId": "1d7ab552-a45b-4690-e6ed-6220eb156b63"
      },
      "outputs": [],
      "source": [
        "%%writefile parallelLoopEqualChunks.c\n",
        "/* parallelLoopEqualChunks.c\n",
        " * ... illustrates the parallel for loop pattern in MPI\n",
        " *\tin which processes perform the loop's iterations in equal-sized 'chunks'\n",
        " *\t(preferable when loop iterations access memory/cache locations) ...\n",
        " * Joel Adams, Calvin College, November 2009.\n",
        " *    updated by Libby Shoop, Macalester College, 2017\n",
        " *\n",
        " * Usage: mpirun -np N ./parallelForEqualChunks\n",
        " *\n",
        " * Exercise:\n",
        " * - Compile and run, varying N: 1, 2, 4, and 8\n",
        " * - Change REPS to 16, save, recompile, rerun, varying N again.\n",
        " * - Explain how this pattern divides the iterations of the loop\n",
        " *    among the processes.\n",
        " */\n",
        "\n",
        "#include <stdio.h> // printf()\n",
        "#include <mpi.h>   // MPI\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    const int REPS = 8;                      // repetitions in a loop\n",
        "    int id = -1, numProcesses = -1;\n",
        "\n",
        "    MPI_Init(&argc, &argv);\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &id);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n",
        "\n",
        "    // In this example, ensure that the REPS can ben evenly divided by the\n",
        "    // number of processors and that the number of processes doesn't exceed REPS.\n",
        "    // If either is the case, stop.\n",
        "    if ((REPS % numProcesses) == 0 && numProcesses <= REPS) {\n",
        "\n",
        "      int chunkSize = REPS / numProcesses;      // find chunk size\n",
        "      int start = id * chunkSize;               // find starting index\n",
        "      int stop = start + chunkSize;             // find stopping index\n",
        "\n",
        "      for (int i = start; i < stop; i++) {      // iterate through our range\n",
        "          printf(\"Process %d is performing iteration %d\\n\", id, i);\n",
        "      }\n",
        "\n",
        "    } else {\n",
        "      if (id == 0) {\n",
        "          printf(\"Please run with -np divisible by and less than or equal to %d\\n.\", REPS);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBshPRYXLLhJ",
        "outputId": "113155eb-a2d7-4ac0-ca6f-22fe20f550c2"
      },
      "outputs": [],
      "source": [
        "!mpicc -Wall -ansi -pedantic -std=c99 -o parallelLoopEqualChunks parallelLoopEqualChunks.c \n",
        "!mpirun --allow-run-as-root -np 4 ./parallelLoopEqualChunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ_ThE3-KthZ"
      },
      "source": [
        "### Exercises\n",
        "\n",
        "- Run, using these numbers of processes, N: 1, 2, 4, and 8 (i.e., vary the  argument to -np).\n",
        "- Change REPS to 16 in the code and rerun it. Then rerun with mpirun, varying N again.\n",
        "- Explain how this pattern divides the iterations of the loop among the processes.\n",
        "\n",
        "Which of the following is the correct assignment of loop iterations to processes for this code, when REPS is 8 and numProcesses is 4?\n",
        "\n",
        "\n",
        "![picture](images/decomp_choices.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_okom_QiQeHe"
      },
      "source": [
        "## Parallel for Loop Program Structure: chunks of 1\n",
        "\n",
        "In the code below, we again use the variable called `REPS` for the total amount or work, or repetitions, that the for loop is accomplishing. This particular code is designed so that the number of repetitions should be more than or equal to the number of processes requested.\n",
        ".. note:: Typically in real problems, the number of repetitions is much higher than the number of processes. We keep it small here to illustrate what is happening.\n",
        "\n",
        "Like the last example all processes execute the code in the part of the if statement that evaluates to True. Note that in the for loop in this case we simply have process whose id is 0 start at iteration 0, then skip to 0 + numProcesses for its next iteration, and so on. Similarly, process 1 starts at iteration 1, skipping next to 1+ numProcesses, and continuing until REPs is reached. Each process performs similar single 'slices' or 'chunks of size 1' of the whole loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15B2xax5RXEY",
        "outputId": "47935fb4-317a-4940-90a9-ee0a687083a3"
      },
      "outputs": [],
      "source": [
        "%%writefile parallelLoopChunksOf1.c\n",
        "/* parallelLoopChunksOf1.c\n",
        " * ... illustrates the parallel for loop pattern in MPI\n",
        " *\tin which processes perform the loop's iterations in 'chunks'\n",
        " *      of size 1 (simple, and useful when loop iterations\n",
        " *      do not access memory/cache locations) ...\n",
        " * Note this is much simpler than the 'equal chunks' loop.\n",
        " * Joel Adams, Calvin College, November 2009.\n",
        " *   updated by Libby Shoop, Macalester College, July, 2017\n",
        " *\n",
        " * Usage: mpirun -np N ./parallelLoopChunksOf1\n",
        " *\n",
        " * Exercise:\n",
        " * - Compile and run, varying N: 1, 2, 3, 4, 5, 6, 7, 8\n",
        " * - Change REPS to 16, save, recompile, rerun, varying N again.\n",
        " * - Explain how this pattern divides the iterations of the loop\n",
        " *    among the processes.\n",
        " */\n",
        "\n",
        "#include <stdio.h>  // printf()\n",
        "#include <mpi.h>    // MPI\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    const int REPS = 8;\n",
        "    int id = -1, numProcesses = -1, i = -1;\n",
        "\n",
        "    MPI_Init(&argc, &argv);\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &id);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n",
        "\n",
        "    if (numProcesses > REPS) {\n",
        "      if (id == 0) {\n",
        "          printf(\"Please run with -np less than or equal to %d\\n.\", REPS);\n",
        "      }\n",
        "    } else {\n",
        "      for (i = id; i < REPS; i += numProcesses) {\n",
        "          printf(\"Process %d is performing iteration %d\\n\", id, i);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0ewx6nLSLgV",
        "outputId": "2a2f9d94-849c-4de3-e8e5-c9b9f2940b5f"
      },
      "outputs": [],
      "source": [
        "!mpicc -Wall -ansi -pedantic -std=c99 -o parallelLoopChunksOf1 parallelLoopChunksOf1.c \n",
        "!mpirun --allow-run-as-root -np 4 ./parallelLoopChunksOf1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vww7AO93S8QD"
      },
      "source": [
        "### Exercises\n",
        "- Run, using these numbers of processes, N: 1, 2, 4, and 8\n",
        "- Compare source code to output.\n",
        "- Change REPS to 16, save, rerun, varying N again.\n",
        "- Explain how this pattern divides the iterations of the loop among the processes.\n",
        "\n",
        "Which of the following is the correct assignment of loop iterations to processes for this code, when REPS is 8 and numProcesses is 4?\n",
        "\n",
        "\n",
        "![picture](images/decomp_choices.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGj_Em7xToeB"
      },
      "source": [
        "# Point to point communication: the message passing pattern\n",
        "\n",
        "The fundamental basis of coordination between independent processes is point-to-point communication between processes through the communication links in the MPI.COMM_WORLD. The form of communication is called message passing, where one process **sends** data to another one, who in turn must **receive** it from the sender. This is illustrated as follows:\n",
        "\n",
        "![picture](images/send_recv.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXSiO5dGuJda"
      },
      "source": [
        "## Message Passing Pattern: Key Problem\n",
        "\n",
        "The following code represents a common error that many programmers have inadvertently placed in their code. The concept behind this program is that we wish to use communication between pairs of processes, like this:\n",
        "\n",
        "![picture](images/pair_exchange.png)\n",
        "\n",
        "For message passing to work between a pair of processes, one must send and the other must receive. If we wish to **exchange** data, then each process will need to perform both a send and a receive.\n",
        "The idea is that process 0 will send data to process 1, who will receive it from process 0. Process 1 will also send some data to process 0, who will receive it from process 1. Similarly, processes 2 and 3 will exchange messages: process 2 will send data to process 3, who will receive it from process 2. Process 3 will also send some data to process 2, who will receive it from process 3.\n",
        "\n",
        "If we have more processes, we still want to pair up processes together to exchange messages. The mechanism for doing this is to know your process id. If your id is odd (1, 3 in the above diagram), you will send and receive from your neighbor whose id is id - 1. If your id is even (0, 2), you will send and receive from your neighbor whose id is id + 1. This should work even if we add more than 4 processes, as long as the number of processes is divisible by 2.\n",
        "\n",
        "![warning sign](images/warning.png)\n",
        "**Warning** There is a problem with the following code called *deadlock*. This happens when every process is waiting on an action from another process. The program cannot complete. **To stop the program, choose the small square that appears after you choose to run the mpirun cell.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_-_ypqDvAM0",
        "outputId": "a59a1fa5-100e-4a58-91d1-a1a724e5d874"
      },
      "outputs": [],
      "source": [
        "%%writefile messagePassingDeadlock.c\n",
        "/* messagePassingDeadlock.c\n",
        " * ... illustrates deadlock with MPI_Send() and MPI_Recv() commands...\n",
        " *\n",
        " * Joel Adams, Calvin College, November 2009.\n",
        " * Modified by Hannah Sonsalla, Macalester College 2017.\n",
        " *\n",
        " * Usage: mpirun -np N ./messagePassing\n",
        " *\n",
        " * Exercise:\n",
        " * - Compile and run, using more than one process.\n",
        " * - Use source code to trace execution.\n",
        " * - Why does this fail?\n",
        " */\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <mpi.h>\n",
        "\n",
        "int odd(int number) { return number % 2; }\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    int id = -1, numProcesses = -1;\n",
        "    int sendValue = -1, receivedValue = -1;\n",
        "    MPI_Status status;\n",
        "\n",
        "    MPI_Init(&argc, &argv);\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &id);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n",
        "\n",
        "    if (numProcesses > 1) {\n",
        "        sendValue = id;\n",
        "        if ( odd(id) ) {  // odd processors receive from their 'left neighbor', then send\n",
        "            MPI_Recv(&receivedValue, 1, MPI_INT, id-1, 2,\n",
        "                       MPI_COMM_WORLD, &status);\n",
        "            MPI_Send(&sendValue, 1, MPI_INT, id-1, 1, MPI_COMM_WORLD);\n",
        "\n",
        "        } else {          // even processors receive from their 'right neighbor', then send\n",
        "            MPI_Recv(&receivedValue, 1, MPI_INT, id+1, 1,\n",
        "                       MPI_COMM_WORLD, &status);\n",
        "            MPI_Send(&sendValue, 1, MPI_INT, id+1, 2, MPI_COMM_WORLD);\n",
        "        }\n",
        "\n",
        "        printf(\"Process %d of %d computed %d and received %d\\n\",\n",
        "                id, numProcesses, sendValue, receivedValue);\n",
        "    } else if ( !id) {  // only process 0 does this part\n",
        "        printf(\"\\nPlease run this program using -np N where N is positive and even.\\n\\n\");\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbbAxy7vvYwW"
      },
      "outputs": [],
      "source": [
        "!mpicc -Wall -ansi -pedantic -std=c99 -o messagePassingDeadlock messagePassingDeadlock.c \n",
        "!mpirun --allow-run-as-root -np 4 ./messagePassingDeadlock"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MLayXixxraM"
      },
      "source": [
        "![warning sign](images/warning.png)Remember,**To stop the program, choose the small square that appears after you choose to run the mpirun cell.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49L8NojO6axL"
      },
      "source": [
        "#### What causes the deadlock?\n",
        "\n",
        "Each process, regardless of its id, will execute a receive request first. In this model, recv is a **blocking** function- it will not continue until it gets data from a send. So every process is blocked waiting to receive a message.\n",
        "\n",
        "#### Can you think of how to fix this problem?\n",
        "\n",
        "Since recv is a **blocking** function, we need to have some processes send first, while others correspondingly recv first from those who send first. This provides coordinated exchanges.\n",
        "\n",
        "Go to the next example to see the solution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxKI85d8LLll"
      },
      "source": [
        "## Message Passing Patterns: avoiding deadlock\n",
        "\n",
        "Let's look at a few more correct message passing examples.\n",
        "\n",
        "### Fix the Deadlock\n",
        "\n",
        "To fix deadlock of the previous example, we coordinate the communication between pairs of processes so that there is an ordering of sends and receives between them.\n",
        "\n",
        "![Important symbol](images/Important.jpg)**Important:** The new code corrects deadlock with a simple change: odd process sends first, even process receives first. *This is the proper pattern for exchanging data between pairs of processes.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OO8N49wuL0fR",
        "outputId": "566deccb-fad7-44db-fae9-bd92340ba25c"
      },
      "outputs": [],
      "source": [
        "%%writefile messagePassing.c\n",
        "/* messagePassing.c\n",
        " * ... illustrates the use of the MPI_Send() and MPI_Recv() commands...\n",
        " * Joel Adams, Calvin College, November 2009.\n",
        " * Modified by Hannah Sonsalla, Macalester College 2017.\n",
        " *\n",
        " * Usage: mpirun -np N ./messagePassing\n",
        " *\n",
        " * Exercise:\n",
        " * - Compile and run, using N = 4, 6, 8, and 10 processes.\n",
        " * - Use source code to trace execution.\n",
        " * - Explain what each process:\n",
        " * -- sends\n",
        " * -- receives\n",
        " * -- outputs.\n",
        " * - Run using N = 5 processes. What happens?\n",
        " */\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <mpi.h>\n",
        "\n",
        "int odd(int number) { return number % 2; }\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    int id = -1, numProcesses = -1;\n",
        "    int sendValue = -1, receivedValue = -1;\n",
        "    MPI_Status status;\n",
        "\n",
        "    MPI_Init(&argc, &argv);\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &id);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n",
        "\n",
        "    if (numProcesses > 1) {\n",
        "        sendValue = id;\n",
        "        if ( odd(id) ) {  // odd processors send, then receive\n",
        "            MPI_Send(&sendValue, 1, MPI_INT, id-1, 1, MPI_COMM_WORLD);\n",
        "            MPI_Recv(&receivedValue, 1, MPI_INT, id-1, 2,\n",
        "                       MPI_COMM_WORLD, &status);\n",
        "        } else {          // even processors receive, then send\n",
        "            MPI_Recv(&receivedValue, 1, MPI_INT, id+1, 1,\n",
        "                       MPI_COMM_WORLD, &status);\n",
        "            MPI_Send(&sendValue, 1, MPI_INT, id+1, 2, MPI_COMM_WORLD);\n",
        "        }\n",
        "\n",
        "        printf(\"Process %d of %d computed %d and received %d\\n\",\n",
        "                id, numProcesses, sendValue, receivedValue);\n",
        "    } else if ( !id) {  // only process 0 does this part\n",
        "        printf(\"\\nPlease run this program using -np N where N is positive and even.\\n\\n\");\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zl-Ms_kMMql",
        "outputId": "78601a4f-64b7-4142-d2ec-dc21c58b6466"
      },
      "outputs": [],
      "source": [
        "!mpicc -Wall -ansi -pedantic -std=c99 -o messagePassing messagePassing.c \n",
        "!mpirun --allow-run-as-root -np 4 ./messagePassing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKgBEDizMkRM"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "- Run, using N = 4, 6, 8, and 10 processes. (Note what happens if you use an odd number instead.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4ybJILhM137"
      },
      "source": [
        "## Sending data structures\n",
        "This next example illustrates that we can exchange different arrays of data between processes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grQWl82lNWsn",
        "outputId": "fc82d10f-3118-4f4d-c480-21e6e68fcab6"
      },
      "outputs": [],
      "source": [
        "%%writefile messagePassing2.c\n",
        "/* messagePassing2.c\n",
        " * ... illustrates using MPI_Send() and MPI_Recv() commands on arrays...\n",
        " * While this example sends and receives char arrays (strings),\n",
        " *  the same approach works on arrays of numbers or other types.\n",
        " * Joel Adams, Calvin College, September 2013.\n",
        " *\n",
        " * Usage: mpirun -np N ./messagePassing2\n",
        " *\n",
        " * Exercise:\n",
        " * - Compile and run, varying N: 1, 2, 4, 8.\n",
        " * - Trace execution using source code.\n",
        " * - Compare to messagePassing1.c; note send/receive differences.\n",
        " */\n",
        "\n",
        "#include <stdio.h>   // printf()\n",
        "#include <mpi.h>     // MPI\n",
        "#include <stdlib.h>  // malloc()\n",
        "#include <string.h>  // strlen()\n",
        "\n",
        "int odd(int number) { return number % 2; }\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    int id = -1, numProcesses = -1, length = -1;\n",
        "    char * sendString = NULL;\n",
        "    char * receivedString = NULL;\n",
        "    char hostName[MPI_MAX_PROCESSOR_NAME];\n",
        "    MPI_Status status;\n",
        "    const int SIZE = (32+MPI_MAX_PROCESSOR_NAME) * sizeof(char);\n",
        "\n",
        "    MPI_Init(&argc, &argv);\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &id);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n",
        "    MPI_Get_processor_name (hostName, &length);\n",
        "\n",
        "    if (numProcesses > 1 && !odd(numProcesses) ) {\n",
        "        sendString = (char*) malloc( SIZE );\n",
        "        receivedString = (char*) malloc( SIZE );\n",
        "        // sprintf: write to string\n",
        "        sprintf(sendString, \"Process %d is on host \\\"%s\\\"\", id, hostName);\n",
        "\n",
        "        if ( odd(id) ) {  // odd processes send, then receive\n",
        "            MPI_Send(sendString, strlen(sendString)+1,\n",
        "                       MPI_CHAR, id-1, 1, MPI_COMM_WORLD);\n",
        "            MPI_Recv(receivedString, SIZE, MPI_CHAR, id-1, 2,\n",
        "                       MPI_COMM_WORLD, &status);\n",
        "        } else {          // even processes receive, then send\n",
        "            MPI_Recv(receivedString, SIZE, MPI_CHAR, id+1, 1,\n",
        "                       MPI_COMM_WORLD, &status);\n",
        "            MPI_Send(sendString, strlen(sendString)+1,\n",
        "                       MPI_CHAR, id+1, 2, MPI_COMM_WORLD);\n",
        "        }\n",
        "\n",
        "        printf(\"\\nProcess %d of %d received the message:\\n\\t'%s'\\n\",\n",
        "                id, numProcesses, receivedString);\n",
        "\n",
        "        free(sendString);\n",
        "        free(receivedString);\n",
        "    } else if ( !id) {  // only process 0 does this part\n",
        "        printf(\"\\nPlease run this program using -np N where N is positive and even.\\n\\n\");\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zYlFcz8Nnx1",
        "outputId": "88b32775-ddad-4b75-c2c1-2d7b01ad0b09"
      },
      "outputs": [],
      "source": [
        "!mpicc -Wall -ansi -pedantic -std=c99 -o messagePassing2 messagePassing2.c \n",
        "!mpirun --allow-run-as-root -np 4 ./messagePassing2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFx0VyZzN1Aw"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "- Run, using N = 4, 6, 8, and 10 processes. \n",
        "- In the above code, locate where the array of elements to be sent is being made by each process. What is different about each array per process?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJQfgyqlOTCr"
      },
      "source": [
        "## Ring of passed messages\n",
        "Another pattern that appears in message passing programs is to use a ring of processes, where messages get sent in this fashion:\n",
        "\n",
        "![picture of ring of message passing](images/ring.png)\n",
        "\n",
        "When we have 4 processes, the idea is that process 0 will send data to process 1, who will receive it from process 0 and then send it to process 2, who will receive it from process 1 and then send it to process 3, who will receive it from process 2 and then send it back around to process 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4PQNTd3PAGR",
        "outputId": "ddff9853-3d7f-479f-f043-0e616885d36d"
      },
      "outputs": [],
      "source": [
        "%%writefile messagePassing3.c\n",
        "/* messagePassing3.c\n",
        " * ... illustrates the use of MPI_Send() and MPI_Recv(),\n",
        " *      in combination with the master-worker pattern.\n",
        " *\n",
        " * Joel Adams, Calvin College, November 2009.\n",
        " *\n",
        " * Usage: mpirun -np N ./messagePassing3\n",
        " *\n",
        " * Exercise:\n",
        " * - Run the program, varying the value of N from 1-8.\n",
        " * - Explain the behavior you observe.\n",
        " */\n",
        "\n",
        "#include <stdio.h>    // printf()\n",
        "#include <string.h>   // strlen()\n",
        "#include <mpi.h>      // MPI\n",
        "\n",
        "#define MAX 256\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    int id = -1, numProcesses = -1;\n",
        "    char sendBuffer[MAX] = {'\\0'};\n",
        "    char recvBuffer[MAX] = {'\\0'};\n",
        "    MPI_Status status;\n",
        "\n",
        "    MPI_Init(&argc, &argv);\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &id);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n",
        "\n",
        "    if (numProcesses > 1) {\n",
        "        if ( id == 0 ) {                              // master:\n",
        "            sprintf(sendBuffer, \"%d\", id);            //  create msg\n",
        "\n",
        "            MPI_Send(sendBuffer,                      //  msg sent\n",
        "                      strlen(sendBuffer) + 1,         //  num chars + NULL\n",
        "                      MPI_CHAR,                       //  type\n",
        "                      id+1,                           //  destination\n",
        "                      1,                              //  tag\n",
        "                      MPI_COMM_WORLD);                //  communicator\n",
        "\n",
        "            MPI_Recv(recvBuffer,                      //  msg received\n",
        "                      MAX,                            //  buffer size\n",
        "                      MPI_CHAR,                       //  type\n",
        "                      numProcesses-1,                 //  sender\n",
        "                      1,                              //  tag\n",
        "                      MPI_COMM_WORLD,                 //  communicator\n",
        "                      &status);                       //  recv status\n",
        "\n",
        "            printf(\"Process #%d of %d received %s\\n\", // show msg\n",
        "                    id, numProcesses, recvBuffer);\n",
        "        } else {                                      // workers:\n",
        "            MPI_Recv(recvBuffer,                      //  msg received\n",
        "                      MAX,                            //  buffer size\n",
        "                      MPI_CHAR,                       //  type\n",
        "                      MPI_ANY_SOURCE,                 //  sender (anyone)\n",
        "                      1,                              //  tag\n",
        "                      MPI_COMM_WORLD,                 //  communicator\n",
        "                      &status);                       //  recv status\n",
        "\n",
        "            printf(\"Process #%d of %d received %s\\n\", // show msg\n",
        "                    id, numProcesses, recvBuffer);\n",
        "\n",
        "            // build msg to send by appending id to msg received\n",
        "            sprintf(sendBuffer, \"%s %d\", recvBuffer, id);\n",
        "\n",
        "            MPI_Send(sendBuffer,                      //  msg to send\n",
        "                      strlen(sendBuffer) + 1,         //  num chars + NULL\n",
        "                      MPI_CHAR,                       //  type\n",
        "                      (id+1) % numProcesses,          //  destination\n",
        "                      1,                              //  tag\n",
        "                      MPI_COMM_WORLD);                //  communicator\n",
        "        }\n",
        "    } else {\n",
        "        printf(\"\\nPlease run this program with at least 2 processes\\n\\n\");\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGTpIE-pPRIq",
        "outputId": "7026fadc-8767-4774-d5cb-46afd668c8c8"
      },
      "outputs": [],
      "source": [
        "!mpicc -Wall -ansi -pedantic -std=c99 -o messagePassing3 messagePassing3.c \n",
        "!mpirun --allow-run-as-root -np 4 ./messagePassing3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6bvSqgMPeyt"
      },
      "source": [
        "### Exercises\n",
        "- Run, using N = from 1 through 8 processes.\n",
        "- Make sure that you can trace how the code generates the output that you see.\n",
        "- How is the finishing of the 'ring' completed, where the last process determines that it should send back to process 0?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPqOxvbK5qSx"
      },
      "source": [
        "# Collective Communication: Broadcast pattern\n",
        "There are many cases when a master process obtains or creates data that needs to be sent to all of the other processes. There is a special pattern for this called **broadcast**. You will see examples of the master sending different types of data to each of the other processes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57wzDFu2xxvW"
      },
      "source": [
        "## Broadcast from master to workers\n",
        "\n",
        "We will look at three types of data that can be created in the master and sent to the workers. Rather than use send and receive, we will use a special new function called bcast.\n",
        "\n",
        "![Important symbol](images/Important.jpg) **Note:** In each code example, note how the master does one thing, and the workers do another, but **all of the processes execute the bcast function.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3eZ1sN1yVsK"
      },
      "source": [
        "### Broadcast an integer\n",
        "\n",
        "Find the place in this code where the data is being broadcast to all of the processes. Match the prints to the output you observe when you run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVx87ecmynYO",
        "outputId": "eb94261d-4033-4e6a-946f-7efd0080c2cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing broadcast.c\n"
          ]
        }
      ],
      "source": [
        "%%writefile broadcast.c\n",
        "/* broadcast.c\n",
        " * ... illustrates the use of MPI_Bcast() with a scalar value...\n",
        " *      (compare to array version).\n",
        " * Joel Adams, Calvin College, April 2016.\n",
        " *\n",
        " * Usage: mpirun -np N ./broadcast\n",
        " *\n",
        " * Exercise:\n",
        " * - Compile and run several times,\n",
        " *     using 2, 4, and 8 processes\n",
        " * - Use source code to trace execution and output\n",
        " *     (noting contents of file \"data.txt\");\n",
        " * - Explain behavior/effect of MPI_Bcast().\n",
        " */\n",
        "\n",
        "#include <mpi.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <assert.h>\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "\tint answer = 0;\n",
        "\tint numProcs = 0, myRank = 0;\n",
        "\n",
        "\tMPI_Init(&argc, &argv);\n",
        "\tMPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n",
        "\tMPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n",
        "\n",
        "\tif (myRank == 0) {\n",
        "\t\tanswer = 42;\n",
        "\t}\n",
        "\n",
        "\tprintf(\"BEFORE the broadcast, process %d's answer = %d\\n\",\n",
        "\tmyRank, answer);\n",
        "\n",
        "\tMPI_Bcast(&answer, 1, MPI_INT, 0, MPI_COMM_WORLD);\n",
        "\n",
        "\tprintf(\"AFTER the broadcast, process %d's answer = %d\\n\",\n",
        "\tmyRank, answer);\n",
        "\n",
        "\tMPI_Finalize();\n",
        "\n",
        "\treturn 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SK268l7SzB5e",
        "outputId": "96df228c-f1c5-411b-dff6-d948f856b45a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BEFORE the broadcast, process 1's answer = 0\n",
            "BEFORE the broadcast, process 0's answer = 42\n",
            "AFTER the broadcast, process 0's answer = 42\n",
            "AFTER the broadcast, process 1's answer = 42\n"
          ]
        }
      ],
      "source": [
        "!mpicc -Wall -ansi -pedantic -std=c99 -o broadcast broadcast.c \n",
        "!mpirun --allow-run-as-root -np 4 ./broadcast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNe1kQLpyqh2"
      },
      "source": [
        "#### Exercise\n",
        "- Run, using N = from 1 through 8 processes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcckAX8G0Pd-"
      },
      "source": [
        "### Broadcast user input\n",
        "\n",
        "The following program will take extra input that will get broadcast to all processes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uI7oPCn309N_",
        "outputId": "f41fb72b-287c-45e3-a39e-65bdea3fefce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing broadcastUserInput.c\n"
          ]
        }
      ],
      "source": [
        "%%writefile broadcastUserInput.c\n",
        "/* broadcastUserInput.c\n",
        " * ... illustrates the use of MPI_Bcast() with a scalar value\n",
        " *     obtained via a command line argument.\n",
        " *\n",
        " * Hannah Sonsalla, Macalester College 2017\n",
        " * Modeled from code by Joel Adams, Calvin College, April 2016.\n",
        " *\n",
        " * Usage: mpirun -np N ./broadcastUserInput <integer>\n",
        " *\n",
        " * Exercise:\n",
        " * - Compile and run several times varying the number\n",
        " *   of processes and integer value\n",
        " * - Explain the behavior you observe\n",
        " */\n",
        "\n",
        "#include <mpi.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <assert.h>\n",
        "\n",
        "#define MASTER 0\n",
        "\n",
        "/* gets value of answer from user\n",
        " * @param: argc, argument count.\n",
        " * @param: argv, argument pointer array.\n",
        " * @param: myRank, rank of current process\n",
        " * @param: answer, variable to store value given by user\n",
        " * Precondition: argc is a count of the number of arguments.\n",
        " *              && argv is a pointer array that points to the arguments.\n",
        " *              && myRank is the rank of this MPI process.\n",
        " *\t\t&& answer is the variable to be assigned value.\n",
        " * Postcondition: answer has been filled with value from user\n",
        " *                if given, else answer remains set to 0.\n",
        " */\n",
        "void getInput(int argc, char* argv[], int myRank, int* answer) {\n",
        "\n",
        "    if (myRank == 0){  // master process\n",
        "        if (argc == 2){\n",
        "             *answer = atoi(argv[1]);\n",
        "        }\n",
        "    }\n",
        "    MPI_Bcast(answer, 1, MPI_INT, 0, MPI_COMM_WORLD);\n",
        "}\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    int answer = 0, length = 0;\n",
        "    int myRank = 0;\n",
        "\n",
        "    char myHostName[MPI_MAX_PROCESSOR_NAME];\n",
        "\n",
        "    MPI_Init(&argc, &argv);\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n",
        "    MPI_Get_processor_name (myHostName, &length);\n",
        "\n",
        "    printf(\"BEFORE the broadcast, process %d on host '%s' has answer = %d\\n\",\n",
        "             myRank, myHostName, answer);\n",
        "\n",
        "    getInput(argc, argv, myRank, &answer);\n",
        "\n",
        "    printf(\"AFTER the broadcast, process %d on host '%s' has answer = %d\\n\",\n",
        "             myRank, myHostName, answer);\n",
        "\n",
        "    MPI_Finalize();\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUUqC5t11qJW"
      },
      "source": [
        "![warning sign](images/warning.png)\n",
        "**Warning** This program is unlike any of the others and takes in a second argument, as shown below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzeQF4rB1aa3",
        "outputId": "d21b6018-a855-4a0d-81e8-9ff1897f3408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BEFORE the broadcast, process 1 on host 'ip-172-31-58-111' has answer = 0\n",
            "BEFORE the broadcast, process 0 on host 'ip-172-31-58-111' has answer = 0\n",
            "AFTER the broadcast, process 0 on host 'ip-172-31-58-111' has answer = 42\n",
            "AFTER the broadcast, process 1 on host 'ip-172-31-58-111' has answer = 42\n"
          ]
        }
      ],
      "source": [
        "!mpicc -Wall -ansi -pedantic -std=c99 -o broadcastUserInput broadcastUserInput.c \n",
        "!mpirun --allow-run-as-root -np 4 ./broadcastUserInput 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocfu_ZYw3QCz"
      },
      "source": [
        "#### Exercise\n",
        "- Run, using N = from 1 through 8 processes, with an integer of your choosing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gcBP4q93eCj"
      },
      "source": [
        "### Broadcast an array\n",
        "\n",
        "This is just one more example to show that other data structures can also be broadcast from the master to all worker processes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8k_sECN3xCv",
        "outputId": "ab68977a-a73e-458e-eb7b-992e3782e1a6"
      },
      "outputs": [],
      "source": [
        "%%writefile broadcastSendReceive.c\n",
        "/*\n",
        " * broadcastSendReceive.c\n",
        " * ... illustrates basic send receive functions.\n",
        " * Master process sends filled array to each process.\n",
        " *\n",
        " * Hannah Sonsalla, Macalester College 2017\n",
        " * fill and print function from code by Joel Adams, Calvin College\n",
        " *\n",
        " * Usage: mpirun -np N ./broadcastSendReceive\n",
        " *\n",
        " * Exercise:\n",
        " * - Compile and run, using 2, 4, and 8 processes\n",
        " * - Use source code to trace execution and output\n",
        " * \n",
        " */\n",
        "\n",
        "#include <mpi.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "/* fill an array with some arbitrary values \n",
        " * @param: a, an int*.\n",
        " * @param: size, an int.\n",
        " * Precondition: a is the address of an array of ints.\n",
        " *              && size is the number of ints a can hold.\n",
        " * Postcondition: a has been filled with arbitrary values \n",
        " *                { 11, 12, 13, ... }.\n",
        " */\n",
        "void fill(int* a, int size) {\n",
        "\tint i;\n",
        "\tfor (i = 0; i < size; i++) {\n",
        "\t\ta[i] = i+11;\n",
        "\t}\n",
        "}\n",
        "\n",
        "/* display a string, a process id, and its array values \n",
        " * @param: str, a char*\n",
        " * @param: id, an int\n",
        " * @param: a, an int*.\n",
        " * Precondition: str points to either \"BEFORE\" or \"AFTER\"\n",
        " *              && id is the rank of this MPI process\n",
        " *              && a is the address of an 8-element int array.\n",
        " * Postcondition: str, id, and a have all been written to stdout.\n",
        " */\n",
        "void print(char* str, int id, int* a) {\n",
        "\tprintf(\"%s array sent, process %d has: {%d, %d, %d, %d, %d, %d, %d, %d}\\n\",\n",
        "\t   str, id, a[0], a[1], a[2], a[3], a[4], a[5], a[6], a[7]);\n",
        "}\n",
        "\n",
        "#define MAX 8\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "\tint id = -1, numProcesses = -1;\n",
        "\tint array[MAX] = {0};\n",
        "    \n",
        "\n",
        "\tMPI_Init(&argc, &argv);\n",
        "\tMPI_Comm_rank(MPI_COMM_WORLD, &id);\n",
        "    \tMPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n",
        "    \n",
        "\tif (id == 0) fill(array, MAX);\n",
        "     \n",
        "\tprint(\"BEFORE\", id, array);\n",
        "\t\n",
        "\t// master process sends array to every process\n",
        "\tif (id == 0) {\n",
        "\t\tfor (int i = 1; i < numProcesses; i++) {\n",
        "\t\t\tMPI_Send(&array, MAX, MPI_INT, \n",
        "\t\t\t    i, 1, MPI_COMM_WORLD);\n",
        "\t    }\n",
        "\t}\n",
        "\t\n",
        "\telse {\n",
        "\t    MPI_Recv(&array, MAX, MPI_INT, 0, \n",
        "\t        1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
        "\t}\n",
        "\t\n",
        "    \tprint(\"AFTER\", id, array);\n",
        " \tMPI_Finalize();\n",
        "\n",
        "\treturn 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSGPEJ244FdZ",
        "outputId": "95b0594a-3843-4c43-93ff-b478a55f8f83"
      },
      "outputs": [],
      "source": [
        "!mpicc -Wall -ansi -pedantic -std=c99 -o broadcastSendReceive broadcastSendReceive.c \n",
        "!mpirun --allow-run-as-root -np 4 ./broadcastSendReceive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGWlHIPh4t08"
      },
      "source": [
        "#### Exercise\n",
        "- Run, using N = from 1 through 8 processes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogLec1ig6HZj"
      },
      "source": [
        "# Collective Communication: reduction pattern\n",
        "\n",
        "There are often cases when every process needs to complete a partial result of an overall computation. For example if you want to process a large set of numbers by summing them together into one value (i.e. *reduce* a set of numbers into one value, its sum), you could do this faster by having each process compute a partial sum, then have all the processes communicate to add each of their partial sums together.\n",
        "\n",
        "This is so common in parallel processing that there is a special collective communication function called **reduce** that does just this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwiw83d44_gR"
      },
      "source": [
        "## Collective Communication: reduce function\n",
        "\n",
        "The type of reduction of many values down to one can be done with different types of operators on the set of values computed by each process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrwM90WO6jKv"
      },
      "source": [
        "### Reduce all values using sum and max\n",
        "In this example, every process computes the square of (id+1). Then all those values are summed together and also the maximum function is applied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jPJPMgE5U_t",
        "outputId": "8f7cfa88-18a7-48ae-ff61-409fea63caaa"
      },
      "outputs": [],
      "source": [
        "%%writefile reduction.c\n",
        "/* reduction.c\n",
        "* ... illustrates the use of MPI_Reduce()...\n",
        "* Joel Adams, Calvin College, November 2009.\n",
        "*\n",
        "* Usage: mpirun -np N ./reduction\n",
        "*\n",
        "* Exercise:\n",
        "* - Compile and run, varying N: 4, 6, 8, 10.\n",
        "* - Explain behavior of MPI_Reduce().\n",
        "*/\n",
        "\n",
        "#include <mpi.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    int numProcs = -1, myRank = -1, square = -1, max = -1, sum = 0;\n",
        "\n",
        "    MPI_Init(&argc, &argv);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n",
        "\n",
        "    square = (myRank+1) * (myRank+1);\n",
        "\n",
        "    printf(\"Process %d computed %d\\n\", myRank, square);\n",
        "\n",
        "    MPI_Reduce(&square, &sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n",
        "\n",
        "    MPI_Reduce(&square, &max, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n",
        "\n",
        "    if (myRank == 0) {\n",
        "        printf(\"\\nThe sum of the squares is %d\\n\\n\", sum);\n",
        "        printf(\"The max of the squares is %d\\n\\n\", max);\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xAazDP17GJJ",
        "outputId": "29e2feff-a2ae-4950-e6bd-ed8d35378732"
      },
      "outputs": [],
      "source": [
        "!mpicc -Wall -ansi -pedantic -std=c99 -o reduction reduction.c \n",
        "!mpirun --allow-run-as-root -np 4 ./reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRLLMm9j7iOA"
      },
      "source": [
        "#### Exercises\n",
        "- Run, using N = from 1 through 8 processes.\n",
        "- Try replacing MPI.MAX with MPI.MIN(minimum) and/or replacing MPI.SUM with MPI.PROD (product). Then save and run the code again.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnO-S8V37tpD"
      },
      "source": [
        "### Reduction on an array of values\n",
        "\n",
        "Here we will use reduction with arrays of values. Then note how you can change the semantics in the exercises.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsAow5Ds8DAv",
        "outputId": "36562141-fb8e-4081-9456-bb3a42987fff"
      },
      "outputs": [],
      "source": [
        "%%writefile reduction2.c\n",
        "/* reduction2.c\n",
        " * ... illustrates the use of MPI_Reduce() using arrays...\n",
        " * Joel Adams, Calvin College, January 2015.\n",
        " *\n",
        " * Usage: mpirun -np 4 ./reduction2\n",
        " *\n",
        " * Exercise:\n",
        " * - Compile and run, comparing output to source code.\n",
        " * - Explain behavior of MPI_Reduce() in terms of\n",
        " *     srcArr and destArr.\n",
        " */\n",
        "\n",
        "#include <mpi.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "#define ARRAY_SIZE 5\n",
        "\n",
        "void printArray(int id, char* arrayName, int* array, int SIZE);\n",
        "void printSeparator(char* separator, int id);\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    int myRank = -1;\n",
        "    int srcArr[ARRAY_SIZE] = {0};\n",
        "    int destArr[ARRAY_SIZE] = {0};\n",
        "\n",
        "    MPI_Init(&argc, &argv);\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n",
        "\n",
        "    if (myRank == 0) {\n",
        "        printf(\"\\nBefore reduction: \");\n",
        "        printArray(myRank, \"destArr\", destArr, ARRAY_SIZE);\n",
        "    }\n",
        "\n",
        "    for (unsigned i = 0; i < ARRAY_SIZE; i++) {\n",
        "        srcArr[i] = myRank * i;\n",
        "    }\n",
        "\n",
        "    printSeparator(\"\", myRank);\n",
        "    printArray(myRank, \"srcArr\", srcArr, ARRAY_SIZE);\n",
        "    printSeparator(\"----\", myRank);\n",
        "\n",
        "    MPI_Reduce(srcArr, destArr, ARRAY_SIZE, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n",
        "\n",
        "    if (myRank == 0) {\n",
        "        printf(\"\\nAfter reduction:  \");\n",
        "        printArray(myRank, \"destArr\", destArr, ARRAY_SIZE);\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "/* utility to display an array\n",
        " * params: id, the rank of the current process\n",
        " *         arrayName, the name of the array being displayed\n",
        " *         array, the array being displayed\n",
        " *         SIZE, the number of items in array.\n",
        " * postcondition:\n",
        " *         the id, name, and items in array have been printed to stdout.\n",
        " */\n",
        "void printArray(int id, char* arrayName, int * array, int SIZE) {\n",
        "    printf(\"Process %d, %s: [\", id, arrayName);\n",
        "    for (int i = 0; i < SIZE; i++) {\n",
        "        printf(\"%3d\", array[i]);\n",
        "        if (i < SIZE-1) printf(\",\");\n",
        "    }\n",
        "    printf(\"]\\n\");\n",
        "}\n",
        "\n",
        "/* utility to print a separator string between before and after sections.\n",
        " * params: separator, a string\n",
        " *         id, the rank of the current process.\n",
        " * postcondition: the master process has printed the separator.\n",
        " */\n",
        "void printSeparator(char* separator, int id) {\n",
        "    MPI_Barrier(MPI_COMM_WORLD);\n",
        "    if (id == 0) { printf(\"%s\", separator); }\n",
        "    MPI_Barrier(MPI_COMM_WORLD);\n",
        "}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uow-rYeS8rxc",
        "outputId": "e5d72f2e-6eef-4ffc-a31d-062f91db57bf"
      },
      "outputs": [],
      "source": [
        "!mpicc -Wall -ansi -pedantic -std=c99 -o reduction2 reduction2.c \n",
        "!mpirun --allow-run-as-root -np 4 ./reduction2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P74yRNds9SBx"
      },
      "source": [
        "#### Exercises\n",
        "- Run, using N = from 1 through 4 processes.\n",
        "- Observe the new results and explain the behavior of MPI.SUM and MPI.MAX on an array of values.\n",
        "- Can you define your own custom reduction function? How would you do this?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYFc1mVp-e2r"
      },
      "source": [
        "# Collective Communication: scatter and gather pattern\n",
        "\n",
        "There are often cases when each process can work on some portion of a larger data structure. This can be carried out by having the master process maintain the larger structure and send parts to each of the worker processes, keeping part of the structure on the master. Each process then works on their portion of the data, and then the master can get the completed portions back.\n",
        "\n",
        "This is so common in message passing parallel processing that there are two special collective communication functions called **scatter** and **gather** that handle this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL2CQgIo-xTd"
      },
      "source": [
        "## Collective Communication: scatter and gather arrays\n",
        "\n",
        "When several processes need to work on portions of a data structure, such as a 1-d or 2-d array, at various points in a program, a way to do this is to have one node, usually the master, divide the data structure and send portions to each of the other processes, often keeping one portion for itself. Each process then works on that portion of the data, and then the master can get the completed portions back. This type of coordination is so common that MPI has special patterns for it called **scatter** and **gather**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g5HzFU1--zY"
      },
      "source": [
        "### Scatter Arrays\n",
        "The following diagrams illustrate how scatter using an array works. The master generates values in the array and all processes participate in the scatter:\n",
        "\n",
        "![scatter array diagram](images/scatter_list_java.png)\n",
        "\n",
        "After the scatter is completed, each process has one of the smaller array to work on, like this:\n",
        "\n",
        "![after scatter array diagram](images/after_scatter_list_java.png)\n",
        "\n",
        "In this next code example, an array is generated whose length is as twice the number of processes.\n",
        "\n",
        "![Important symbol](images/Important.jpg) **Note:** In the code below, note how all processes must call the scatter function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FI6C7UH7AiMV",
        "outputId": "ccdcdc95-72d1-4794-83f7-c92f2917cadb"
      },
      "outputs": [],
      "source": [
        "%%writefile scatter.c\n",
        "/* scatter.c\n",
        " * ... illustrates the use of MPI_Scatter()...\n",
        " * Joel Adams, Calvin College, November 2009.\n",
        " *\n",
        " * Usage: mpirun -np N ./scatter\n",
        " *\n",
        " * Exercise:\n",
        " * - Compile and run, varying N: 1, 2, 4, 8\n",
        " * - Trace execution through source code.\n",
        " * - Explain behavior/effect of MPI_Scatter().\n",
        " */\n",
        "\n",
        "#include <mpi.h>      // MPI\n",
        "#include <stdio.h>    // printf(), etc.\n",
        "#include <stdlib.h>   // malloc()\n",
        "\n",
        "void print(int id, char* arrName, int* arr, int arrSize);\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "    const int MAX = 8;\n",
        "    int* arrSend = NULL;\n",
        "    int* arrRcv = NULL;\n",
        "    int numProcs = -1, myRank = -1, numSent = -1;\n",
        "\n",
        "    MPI_Init(&argc, &argv);                            // initialize\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n",
        "\n",
        "    if (myRank == 0) {                                 // master process:\n",
        "        arrSend = (int*) malloc( MAX * sizeof(int) );  //  allocate array1\n",
        "        for (int i = 0; i < MAX; i++) {                //  load with values\n",
        "            arrSend[i] = (i+1) * 11;\n",
        "        }\n",
        "        print(myRank, \"arrSend\", arrSend, MAX);        //  display array1\n",
        "    }\n",
        "     \n",
        "    numSent = MAX / numProcs;                          // all processes:\n",
        "    arrRcv = (int*) malloc( numSent * sizeof(int) );   //  allocate array2\n",
        "\n",
        "    MPI_Scatter(arrSend, numSent, MPI_INT, arrRcv,     //  scatter array1 \n",
        "                 numSent, MPI_INT, 0, MPI_COMM_WORLD); //   into array2\n",
        "\n",
        "    print(myRank, \"arrRcv\", arrRcv, numSent);          // display array2\n",
        "\n",
        "    free(arrSend);                                     // clean up\n",
        "    free(arrRcv);\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "void print(int id, char* arrName, int* arr, int arrSize) {\n",
        "    printf(\"Process %d, %s: \", id, arrName);\n",
        "    for (int i = 0; i < arrSize; i++) {\n",
        "        printf(\" %d\", arr[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgt8fHPaA0CH",
        "outputId": "e813b0ea-23e7-4054-bc77-625240a77893"
      },
      "outputs": [],
      "source": [
        "!mpicc -Wall -ansi -pedantic -std=c99 -o scatter scatter.c \n",
        "!mpirun --allow-run-as-root -np 4 ./scatter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9EYqJldBRS-"
      },
      "source": [
        "#### Exercises\n",
        "- Run, using N = from 2 through 8 processes.\n",
        "- If you want to study the code, explain to yourself how the array is generated by the master process and how it gets scattered to the worker processes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIw9YI6GB5go"
      },
      "source": [
        "### Gather Arrays\n",
        "Once several processes have their own array of data, those array elements can also be gathered back together into a larger array, usually in the master process. All processes participate in a gather, like this:\n",
        "\n",
        "![before gather diagram](images/gather_array_java.png)\n",
        "\n",
        "The gather creates a combined array in the master, like this:\n",
        "\n",
        "![after gather diagram](images/after_gather_array_java.png)\n",
        "\n",
        "In this example, each process creates some very small arrays. Then a gather is used to create a combined array on the master process.\n",
        "\n",
        "![Important symbol](images/Important.jpg) **Note:** In the code below, note how all processes must call the gather function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPk2IBX_C46Z",
        "outputId": "8ab9b72a-c830-4f56-b587-1c14e3d0910d"
      },
      "outputs": [],
      "source": [
        "%%writefile gather.c\n",
        "/* gather.c\n",
        " * ... illustrates the use of MPI_Gather()...\n",
        " * Joel Adams, Calvin College, November 2009.\n",
        " *\n",
        " * Usage: mpirun -np N ./gather\n",
        " *\n",
        " * Exercise:\n",
        " * - Compile and run, varying N: 1, 2, 4, 8.\n",
        " * - Trace execution through source.\n",
        " * - Explain behavior of MPI_Gather().\n",
        " */\n",
        "\n",
        "#include <mpi.h>       // MPI\n",
        "#include <stdio.h>     // printf()\n",
        "#include <stdlib.h>    // malloc()\n",
        "\n",
        "void print(int id, char* arrName, int* arr, int arrSize);\n",
        "\n",
        "#define SIZE 3\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "   int  computeArray[SIZE];                          // array1\n",
        "   int* gatherArray = NULL;                          // array2\n",
        "   int  numProcs = -1, myRank = -1,\n",
        "        totalGatheredVals = -1;\n",
        "\n",
        "   MPI_Init(&argc, &argv);                           // initialize\n",
        "   MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n",
        "   MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n",
        "                                                     // all processes:\n",
        "   for (int i = 0; i < SIZE; i++) {                  //  load array1 with\n",
        "      computeArray[i] = myRank * 10 + i;             //   3 distinct values\n",
        "   }\n",
        "\n",
        "   print(myRank, \"computeArray\", computeArray,       //  show array1\n",
        "           SIZE);\n",
        "\n",
        "   if (myRank == 0) {                                // master:\n",
        "      totalGatheredVals = SIZE * numProcs;           //  allocate array2\n",
        "      gatherArray = (int*) malloc( totalGatheredVals * sizeof(int) );\n",
        "   }\n",
        "\n",
        "   MPI_Gather(computeArray, SIZE, MPI_INT,           //  gather array1 vals\n",
        "               gatherArray, SIZE, MPI_INT,           //   into array2\n",
        "               0, MPI_COMM_WORLD);                   //   at master process\n",
        "\n",
        "   if (myRank == 0) {                                // master process:\n",
        "      print(myRank, \"gatherArray\",                   //  show array2\n",
        "             gatherArray, totalGatheredVals);\n",
        "      free(gatherArray);                             // clean up\n",
        "   }\n",
        "\n",
        "\n",
        "   MPI_Finalize();\n",
        "   return 0;\n",
        "}\n",
        "\n",
        "void print(int id, char* arrName, int* arr, int arrSize) {\n",
        "    printf(\"Process %d, %s: \", id, arrName);\n",
        "    for (int i = 0; i < arrSize; i++) {\n",
        "        printf(\" %d\", arr[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvdQljANDOtE",
        "outputId": "224f73ab-bba2-4b7b-d259-f049541ddef5"
      },
      "outputs": [],
      "source": [
        "!mpicc -Wall -ansi -pedantic -std=c99 -o gather gather.c \n",
        "!mpirun --allow-run-as-root -np 4 ./gather"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPVwi5W4DpwJ"
      },
      "source": [
        "#### Exercises\n",
        "- Run, using N = from 2 through 8 processes.\n",
        "- Try with different values of SIZE, perhaps changing printing of result for readability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eCGJi2hEOC1"
      },
      "source": [
        "## Collective Communication:  scatter and gather arrays\n",
        "\n",
        "In this last scatter-gather example, a 1-D array is created by the master, then scattered, using scatter. After each smaller array used by each process is changed, the Gather (capital G) function brings the full array with the changes back into the master.\n",
        "\n",
        "As before, the scatter function is used send portions of a larger array on the master to the workers, like this:\n",
        "\n",
        "![alt text](images/Scatter_array.png)\n",
        "\n",
        "The result of doing this then looks like this, where each process has a portion of the original that they can then work on:\n",
        "\n",
        "![alt text](images/after_Scatter_array.png)\n",
        "\n",
        "The reverse of this process is done using the gather function.\n",
        "\n",
        "In this example,\n",
        "\n",
        "![Important symbol](images/Important.jpg) **Note:** In the code below, note how all processes must call the scatter and gather functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO06o4HAFBZR",
        "outputId": "ae2bf754-8b78-4b76-a7ca-799b79ef6794"
      },
      "outputs": [],
      "source": [
        "%%writefile scatterLoopGather.c\n",
        "/* scatterLoopGather.c\n",
        " * ... scatters an array of data into equal-sized chunks, \n",
        " *      has each process use a loop to double the values in its chunk,\n",
        " *      and then gathers the chunks back to the master process.\n",
        " *\n",
        " * Joel Adams, Calvin University, December 2019.\n",
        " *\n",
        " * Precondition: ARRAY_SIZE is evenly divisible by N\n",
        " *               && N <= ARRAY_SIZE.\n",
        " *\n",
        " * Note: The output of different process's steps will be interleaved\n",
        " *       (even using barriers) b/c stdout is buffered\n",
        " *       and MPI does not guarantee FIFO output behavior.\n",
        " *\n",
        " * Usage: mpirun -np N ./scatterLoopGather\n",
        " *\n",
        " * Exercise:\n",
        " * - Compile and run, using 1, 2, 4, and 8 processes\n",
        " * - Use source code to trace execution and output\n",
        " * - Explain behavior/effect of MPI_Scatter(), MPI_Gather().\n",
        " * - Optional: change ARRAY_SIZE to be another multiple of 8, such as 16\n",
        " * - Optional: add calls to print() to display each array at each step\n",
        " */\n",
        "\n",
        "#include <stdio.h>     // printf\n",
        "#include <stdlib.h>    // malloc, exit, ...\n",
        "#include <mpi.h>       // MPI functionality\n",
        "\n",
        "#define MASTER     0\n",
        "#define ARRAY_SIZE 8\n",
        "\n",
        "void fill(int* a, int size);\n",
        "void printSeparator(const char* separator, int id);\n",
        "void print(char* locLabel, int id, char* aName, int* a, int numElements);\n",
        "\n",
        "/*\n",
        " *  Main function: double the values in an array\n",
        " *  by dividing the work equally among N processes.\n",
        " */\n",
        "int main(int argc, char** argv) {\n",
        "    int* scatterArray = NULL;\n",
        "    int* chunkArray = NULL;\n",
        "    int* gatherArray = NULL;\n",
        "    int numProcs = -1, myRank = -1, chunkSize = -1;\n",
        "\n",
        "    MPI_Init(&argc, &argv);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n",
        "\n",
        "    printSeparator(\"\", myRank);\n",
        "\n",
        "    if (ARRAY_SIZE % numProcs || numProcs > ARRAY_SIZE) {\n",
        "        if (myRank == MASTER) {\n",
        "            printf(\"Please run with -np N divisible by and less than or equal to %d\\n.\", ARRAY_SIZE);\n",
        "        }\n",
        "        MPI_Finalize();\n",
        "        exit(0);\n",
        "    }\n",
        "\n",
        "    if (myRank == MASTER) {     \n",
        "        scatterArray = (int*) malloc( ARRAY_SIZE * sizeof(int) ); // allocate input array\n",
        "        fill(scatterArray, ARRAY_SIZE);                           // populate it \n",
        "        gatherArray = (int*) malloc( ARRAY_SIZE * sizeof(int) );  // allocate result array\n",
        "    }\n",
        "\n",
        "    print(\"BEFORE Scatter\", myRank, \"scatterArray\", scatterArray, ARRAY_SIZE);\n",
        "    \n",
        "    chunkSize = ARRAY_SIZE / numProcs;\n",
        "    chunkArray = (int*) malloc(chunkSize * sizeof(int));          // allocate chunk array\n",
        "\n",
        "    MPI_Scatter(scatterArray, chunkSize, MPI_INT,                 // scatter input array \n",
        "                 chunkArray, chunkSize, MPI_INT, 0, MPI_COMM_WORLD);\n",
        "\n",
        "    print(\"AFTER Scatter\", myRank, \"chunkArray\", chunkArray, chunkSize);\n",
        "\n",
        "    for (unsigned i = 0; i < chunkSize; ++i) {                    // compute using chunk\n",
        "        chunkArray[i] *= 2;\n",
        "    }\n",
        "\n",
        "    print(\"AFTER doubling\", myRank, \"chunkArray\", chunkArray, chunkSize);\n",
        "\n",
        "    MPI_Gather(chunkArray, chunkSize, MPI_INT,                   //  gather chunks\n",
        "                gatherArray, chunkSize, MPI_INT,                 //   into gatherArray\n",
        "                0, MPI_COMM_WORLD);\n",
        "\n",
        "    print(\"AFTER gather\", myRank, \"gatherArray\", gatherArray, ARRAY_SIZE);\n",
        "\n",
        "    free(chunkArray);                                            // everyone clean up\n",
        "    if (myRank == 0) {                                           // master clean up\n",
        "        free(gatherArray); \n",
        "        free(scatterArray);\n",
        "    }\n",
        "\n",
        "    printSeparator(\"\", myRank);\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "\n",
        "/* fill an array with some easy-to-check values\n",
        " * @param: a, an int*.\n",
        " * @param: size, an int.\n",
        " * Precondition: a is the address of an array of ints.\n",
        " *              && size is the number of ints a can hold.\n",
        " * Postcondition: a has been filled with the values\n",
        " *                { 11, 12, 13, ... }.\n",
        " */\n",
        "void fill(int* a, int size) {\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        a[i] = i+11;\n",
        "    }\n",
        "}\n",
        "\n",
        "/* display a separator, synchronizing all processes\n",
        " * @param: separator, a char* \n",
        " * @param: id, an int.\n",
        " * Precondition: separator points to a string to be be displayed\n",
        " *               && id is the MPI rank of this process.\n",
        " * Postcondition: separator has been displayed\n",
        " *                 and all MPI processes have been syncronized.\n",
        " */\n",
        "void printSeparator(const char* separator, int id) {\n",
        "    MPI_Barrier(MPI_COMM_WORLD);\n",
        "    if (id == MASTER) { \n",
        "        printf(\"%s\\n\", separator);\n",
        "    }\n",
        "    MPI_Barrier(MPI_COMM_WORLD);\n",
        "}\n",
        "\n",
        "/* display a string, a process id, and its array values\n",
        " * @param: locLabel, a char*\n",
        " * @param: id, an int\n",
        " * @param: aName, a char*\n",
        " * @param: a, an int*.\n",
        " * @param: numElements, an int.\n",
        " * Precondition: locLabel points to a string describing our location\n",
        " *              && id is the rank of this MPI process\n",
        " *              && aName is the name of the array being printed\n",
        " *              && a is the address of an int array \n",
        " *              && numElements is the number of int-values in a.\n",
        " * Postcondition: str, id, and a have all been written to stdout.\n",
        " */\n",
        "void print(char* locLabel, int id, char* aName, int* a, int numElements) {\n",
        "    printf(\"%s, process %d has this %s: {\", locLabel, id, aName);\n",
        "    if (a != NULL) {\n",
        "        for (int i = 0; i < numElements - 1; ++i) {\n",
        "            printf(\"%d, \", a[i]);\n",
        "        }\n",
        "        printf(\"%d}\\n\", a[numElements - 1]);\n",
        "    } else {\n",
        "        printf(\"}\\n\");\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMIeHGkDFQ1V",
        "outputId": "838f8bb0-6b52-4139-dfc8-a5c921f2bdce"
      },
      "outputs": [],
      "source": [
        "!mpicc -Wall -ansi -pedantic -std=c99 -o scatterLoopGather scatterLoopGather.c \n",
        "!mpirun --allow-run-as-root -np 4 ./scatterLoopGather"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKhQXI7oFRMf"
      },
      "source": [
        "#### Exercises\n",
        "- Run, using N = from 2 through 8 processes.\n",
        "- What are the benefits and limitations of the scatter-gather functionality?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnjHsPScGpEP"
      },
      "source": [
        "# When amount of work varies: balancing the load\n",
        "\n",
        "There are algorithms where the master is used to assign tasks to workers by sending them data and receiving results back as each worker completes a task (or after the worker completes all of its tasks). In many of these cases, the computation time needed by each worker process for each of its tasks can vary somewhat dramatically. This situation is where **dynamic load balancing** can be helpful.\n",
        "\n",
        "In this example we combine the master-worker pattern with message passing. The master has many tasks that need to be completed. The master starts by sending some data needed to complete a task to each worker process. Then the master loops and waits to hear back from each worker by receiving a message from any of them. When the master receives a message from a worker, it sends that worker more data for its next task, unless there are no more tasks to complete, in which case it sends a special message to the worker to stop running.\n",
        "\n",
        "In this simple example, each worker is sent the number of seconds it should 'sleep', which can vary from 1 to 8. This illustrates varying sizes of workloads. Because of the code's simplicity, the number of tasks each worker does doesn't vary by much. In some real examples, the time for one task my be quite different than the time for another, which could have a different outcome, in which some workers were able to complete more tasks as others were doing long ones.\n",
        "\n",
        "This approach can sometimes be an improvement on the assignment of an equal number of tasks to all processes.\n",
        "\n",
        "Note in this case how the master, whose id is 0, handles the assignment of tasks, while the workers simply do what they are sent until they are told to stop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow-7CfeCHY3n",
        "outputId": "c54015c3-75b3-43b9-9b15-2bac38c8102e"
      },
      "outputs": [],
      "source": [
        "%%writefile dynamicLoadBalance.c\n",
        "/* TODO: need to implement this */ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9t30ynerHaaN",
        "outputId": "43447a14-a733-47d7-e464-cc8baac75103"
      },
      "outputs": [],
      "source": [
        "!mpicc -Wall -ansi -pedantic -std=c99 -o dynamicLoadBalance dynamicLoadBalance.c \n",
        "!mpirun --allow-run-as-root -np 4 ./dynamicLoadBalance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t65YHXekHbBK"
      },
      "source": [
        "## Exercises\n",
        "- Run, using N = 4 processes\n",
        "- Study the execution carefully. Note that with 4 processes, 3 are workers. The total number of tasks is 3*4, or 12. Which process does the most work? You can count by looking for the lines that end with \"... from X\", where X is a worker process id.\n",
        "- Try with N = 8 (7 workers)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "kVWqN3EhJFbJ",
        "LJ_ThE3-KthZ",
        "Vww7AO93S8QD",
        "bXSiO5dGuJda",
        "49L8NojO6axL",
        "XxKI85d8LLll",
        "UKgBEDizMkRM",
        "lFx0VyZzN1Aw",
        "eJQfgyqlOTCr",
        "o6bvSqgMPeyt",
        "57wzDFu2xxvW",
        "n3eZ1sN1yVsK",
        "tNe1kQLpyqh2",
        "FcckAX8G0Pd-",
        "ocfu_ZYw3QCz",
        "0gcBP4q93eCj",
        "IGWlHIPh4t08",
        "cwiw83d44_gR",
        "PrwM90WO6jKv",
        "YRLLMm9j7iOA",
        "JnO-S8V37tpD",
        "P74yRNds9SBx",
        "gL2CQgIo-xTd",
        "1g5HzFU1--zY",
        "H9EYqJldBRS-",
        "tIw9YI6GB5go",
        "mPVwi5W4DpwJ",
        "-eCGJi2hEOC1",
        "qKhQXI7oFRMf",
        "t65YHXekHbBK"
      ],
      "name": "C_openmpi_patternlets.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "822ce188d9bce5372c4adbb11364eeb49293228c2224eb55307f4664778e7f56"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}